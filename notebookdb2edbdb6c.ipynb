{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# files_folder_tr = os.listdir('../input/hubmap-kidney-segmentation/train')\n# files_folder_tr = sorted(files_folder_tr)\n# files_folder_te = os.listdir('../input/hubmap-kidney-segmentation/test')\n# files_folder_te = sorted(files_folder_te)\n\n# filename_tr=[]\n# filename_te=[]\n\n# for file_name_tr in files_folder_tr: \n#     filename_tr.append(file_name_tr)\n# for file_name_te in files_folder_te: \n#     filename_te.append(file_name_te)\n    \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-26T21:36:36.074419Z","iopub.execute_input":"2022-09-26T21:36:36.074715Z","iopub.status.idle":"2022-09-26T21:36:36.098248Z","shell.execute_reply.started":"2022-09-26T21:36:36.074684Z","shell.execute_reply":"2022-09-26T21:36:36.097488Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"/kaggle/input/hubmap-kidney-segmentation/sample_submission.csv\n/kaggle/input/hubmap-kidney-segmentation/HuBMAP-20-dataset_information.csv\n/kaggle/input/hubmap-kidney-segmentation/train.csv\n/kaggle/input/hubmap-kidney-segmentation/test/aa05346ff.tiff\n/kaggle/input/hubmap-kidney-segmentation/test/sample_submission.csv\n/kaggle/input/hubmap-kidney-segmentation/test/3589adb90-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/test/2ec3f1bb9.tiff\n/kaggle/input/hubmap-kidney-segmentation/test/3589adb90.tiff\n/kaggle/input/hubmap-kidney-segmentation/test/d488c759a-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/test/57512b7f1-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/test/d488c759a.tiff\n/kaggle/input/hubmap-kidney-segmentation/test/57512b7f1.tiff\n/kaggle/input/hubmap-kidney-segmentation/test/aa05346ff-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/test/2ec3f1bb9-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/0486052bb-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/4ef6695ce-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/e79de561c-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/095bf7a1f.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/0486052bb.json\n/kaggle/input/hubmap-kidney-segmentation/train/095bf7a1f-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/b9a3865fc.json\n/kaggle/input/hubmap-kidney-segmentation/train/26dc41664.json\n/kaggle/input/hubmap-kidney-segmentation/train/afa5e8098.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/e79de561c.json\n/kaggle/input/hubmap-kidney-segmentation/train/8242609fa-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/e79de561c.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/4ef6695ce.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/cb2d976f4-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/0486052bb.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/b9a3865fc-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/aaa6a05cc.json\n/kaggle/input/hubmap-kidney-segmentation/train/b9a3865fc.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/aaa6a05cc.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/2f6ecfcdf-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/2f6ecfcdf.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/afa5e8098-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/54f2eec69.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/8242609fa.json\n/kaggle/input/hubmap-kidney-segmentation/train/4ef6695ce.json\n/kaggle/input/hubmap-kidney-segmentation/train/c68fe75ea.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/54f2eec69.json\n/kaggle/input/hubmap-kidney-segmentation/train/cb2d976f4.json\n/kaggle/input/hubmap-kidney-segmentation/train/1e2425f28.json\n/kaggle/input/hubmap-kidney-segmentation/train/26dc41664-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/8242609fa.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/afa5e8098.json\n/kaggle/input/hubmap-kidney-segmentation/train/1e2425f28.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/c68fe75ea-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/b2dc8411c-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/b2dc8411c.json\n/kaggle/input/hubmap-kidney-segmentation/train/1e2425f28-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/c68fe75ea.json\n/kaggle/input/hubmap-kidney-segmentation/train/b2dc8411c.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/54f2eec69-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/cb2d976f4.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/aaa6a05cc-anatomical-structure.json\n/kaggle/input/hubmap-kidney-segmentation/train/095bf7a1f.json\n/kaggle/input/hubmap-kidney-segmentation/train/26dc41664.tiff\n/kaggle/input/hubmap-kidney-segmentation/train/2f6ecfcdf.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)\nfrom kaggle_datasets import KaggleDatasets\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\nimport os\nimport numpy as np \nimport tensorflow as tf\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport json \nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n# from tensorflow.keras.utils.generic_utils import get_custom_objects\nimport os\nimport pandas as pd\nimport gc ### This is used to free up memory\nimport tifffile ## The input images are in .tiff format and can be parsed using this library\n","metadata":{"execution":{"iopub.status.busy":"2022-09-26T21:31:09.571704Z","iopub.execute_input":"2022-09-26T21:31:09.571909Z","iopub.status.idle":"2022-09-26T21:31:12.355012Z","shell.execute_reply.started":"2022-09-26T21:31:09.571879Z","shell.execute_reply":"2022-09-26T21:31:12.354249Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# to check runing on TPU \n\n\nprint(\"Tensorflow version \" + tf.__version__)\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T21:31:12.356846Z","iopub.execute_input":"2022-09-26T21:31:12.357349Z","iopub.status.idle":"2022-09-26T21:31:12.364981Z","shell.execute_reply.started":"2022-09-26T21:31:12.357310Z","shell.execute_reply":"2022-09-26T21:31:12.364151Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Tensorflow version 2.4.1\nREPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntrain_data = np.array(filename_tr[0:12])\nvalidation_data = np.array(filename_tr[12:15])\ntrain_data\nvalidation_data","metadata":{"execution":{"iopub.status.busy":"2022-09-26T21:37:49.775861Z","iopub.execute_input":"2022-09-26T21:37:49.776226Z","iopub.status.idle":"2022-09-26T21:37:49.783001Z","shell.execute_reply.started":"2022-09-26T21:37:49.776194Z","shell.execute_reply":"2022-09-26T21:37:49.782309Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"array(['2f6ecfcdf-anatomical-structure.json', '2f6ecfcdf.json',\n       '2f6ecfcdf.tiff'], dtype='<U35')"},"metadata":{}}]},{"cell_type":"code","source":"with strategy.scope():\n    def unet_model(OUTPUT_CHANNELS=1, tile_size=512, strides=1):\n        initializer = 'he_normal'\n        #keras.initializers.HeNormal()\n\n        inputs = layers.Input(shape=[tile_size, tile_size, 3])\n\n        ###LEVEL1\n        d_conv1 = layers.Conv2D(filters=64, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(inputs)\n        norm1 = layers.BatchNormalization()(d_conv1)                         \n        d_conv2 = layers.Conv2D(filters=64, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(norm1)                             \n        d_pool1 = layers.MaxPool2D(pool_size=2, strides=2, padding ='same')(d_conv2)\n        ###LEVEL2\n        d_conv3 = layers.Conv2D(filters=128, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_pool1)\n        norm2 = layers.BatchNormalization()(d_conv3)                             \n        d_conv4 = layers.Conv2D(filters=128, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(norm2)\n        d_pool2 = layers.MaxPool2D(pool_size=2, strides=2, padding='same')(d_conv4)\n        ##LEVEL3\n        d_conv5 = layers.Conv2D(filters=256, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_pool2)\n        d_conv6 = layers.Conv2D(filters=256, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_conv5)\n        d_pool3 = layers.MaxPool2D(pool_size=2, strides=2, padding='same')(d_conv6)\n        ###LEVEL4\n        d_conv7 = layers.Conv2D(filters=512, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_pool3)\n        d_conv8 = layers.Conv2D(filters=512, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_conv7)\n        d_pool4 = layers.MaxPool2D(pool_size=2, strides=2, padding='same')(d_conv8)\n        ###LEVEL5\n        d_conv9 = layers.Conv2D(filters=1024, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_pool4)\n        d_conv10 = layers.Conv2D(filters=1024, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_conv9)    \n\n        ###Upsampling\n        ###LEVEL1\n        u_sample1 = layers.UpSampling2D(size=2)(d_conv10)\n        u_conv11 = layers.Conv2D(filters=512, kernel_size=2, strides=strides, padding='same',\n                                 kernel_initializer=initializer)(u_sample1)\n        u_copy1 = layers.Concatenate(axis = 3)([d_conv8, u_conv11])\n\n        u_conv12 = layers.Conv2D(filters=512, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_copy1)\n        u_conv13 = layers.Conv2D(filters=512, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_conv12)\n        ###Level2\n\n        u_sample2 = layers.UpSampling2D(size=2)(u_conv13)\n        u_conv14 = layers.Conv2D(filters=256, kernel_size=2, strides=strides, padding='same',\n                                 kernel_initializer=initializer)(u_sample2)\n        u_copy2 = layers.Concatenate(axis = 3)([d_conv6, u_conv14])\n        u_conv15 = layers.Conv2D(filters=256, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_copy2)\n        u_conv16 = layers.Conv2D(filters=256, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_conv15)\n\n\n        ###Level3\n\n        u_sample3 = layers.UpSampling2D(size=2)(u_conv16)\n        u_conv17 = layers.Conv2D(filters=128, kernel_size=2, strides=strides, padding='same',\n                                 kernel_initializer=initializer)(u_sample3)\n        u_copy3 = layers.Concatenate(axis = 3)([d_conv4, u_conv17])\n        u_conv18 = layers.Conv2D(filters=128, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_copy3)\n\n        u_conv19 = layers.Conv2D(filters=128, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_conv18)\n\n        ###Level4\n\n        u_sample4 = layers.UpSampling2D(size=2)(u_conv19)\n        u_conv20 = layers.Conv2D(filters=64, kernel_size=2, strides=strides, padding='same',\n                                 kernel_initializer=initializer)(u_sample4)\n        u_copy4 = layers.Concatenate(axis = 3)([d_conv2, u_conv20])\n        u_conv21 = layers.Conv2D(filters=64, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_copy4)\n        u_conv22 = layers.Conv2D(filters=64, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_conv21)\n        ###Last Level\n        u_conv23 = layers.Conv2D(filters =1, kernel_size = OUTPUT_CHANNELS, activation='sigmoid')(u_conv22)\n\n        return keras.Model(inputs=inputs, outputs=u_conv23)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T21:31:12.378341Z","iopub.execute_input":"2022-09-26T21:31:12.378705Z","iopub.status.idle":"2022-09-26T21:31:12.400666Z","shell.execute_reply.started":"2022-09-26T21:31:12.378674Z","shell.execute_reply":"2022-09-26T21:31:12.399999Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def dice_coeff(y_true, y_pred):\n        # add epsilon to avoid a divide by 0 error in case a slice has no pixels set\n        # we only care about relative value, not absolute so this alteration doesn't matter\n    _epsilon = 10 ** -7\n    intersections = tf.reduce_sum(y_true * y_pred)\n    unions = tf.reduce_sum(y_true + y_pred)\n    dice_scores = (2.0 * intersections + _epsilon) / (unions + _epsilon)\n    return dice_scores\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\n  \ntf.keras.utils.get_custom_objects().update({\"dice\": dice_loss})   ","metadata":{"execution":{"iopub.status.busy":"2022-09-26T21:31:12.402086Z","iopub.execute_input":"2022-09-26T21:31:12.402348Z","iopub.status.idle":"2022-09-26T21:31:12.411597Z","shell.execute_reply.started":"2022-09-26T21:31:12.402316Z","shell.execute_reply":"2022-09-26T21:31:12.410878Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"image_feature_description = {\n      'image': tf.io.FixedLenFeature([], tf.string),\n      'mask': tf.io.FixedLenFeature([], tf.string),\n      'tile_No': tf.io.FixedLenFeature([], tf.int64),\n      'image_id': tf.io.FixedLenFeature([], tf.string),\n      'start_row_pixel': tf.io.FixedLenFeature([], tf.int64),\n      'start_col_pixel': tf.io.FixedLenFeature([], tf.int64),\n      'image_distribution0': tf.io.FixedLenFeature([], tf.string),\n      'image_distribution1': tf.io.FixedLenFeature([], tf.string)\n      \n  }\n\ndef _parse_image_function2(example_proto):\n  # Parse the input tf.Example proto using the dictionary above.\n    single_example = tf.io.parse_single_example(example_proto, image_feature_description)\n    \n    image_id = single_example['image_id']\n    start_row_pixel = single_example['start_row_pixel']\n    start_col_pixel = single_example['start_col_pixel']\n    \n    num_channels = 3\n    tile_size = 512\n        \n    \n    image =  tf.io.decode_raw(single_example['image'],out_type='uint8')\n   \n    #img_array = tf.reshape( image, ( 1, tile_size, tile_size, num_channels))\n    img_array = tf.reshape( image, (  tile_size, tile_size, num_channels))\n    \n    img_array = tf.cast(img_array, tf.float32) / 255.0\n   \n    mask =  tf.io.decode_raw(single_example['mask'],out_type='bool')\n    \n    mask = tf.reshape(mask, (tile_size,tile_size))\n    \n    mask = tf.cast(mask,tf.float32)\n    \n    image_distribution0 = tf.io.decode_raw(single_example['image_distribution0'], out_type = 'int64')\n    image_distribution1 = tf.io.decode_raw(single_example['image_distribution1'], out_type = 'int64')\n    \n    image_distribution = (image_distribution0, image_distribution1)\n    \n    mtd = dict()\n    mtd['img_index'] = single_example['image_id']\n    mtd['tile_id'] = single_example['tile_No']\n    mtd['start_col_pixel'] = single_example['start_col_pixel']\n    mtd['start_row_pixel'] = single_example['start_row_pixel']\n    struct = {\n        'img_array': img_array,\n        'mask': mask,\n        'mtd': mtd,\n        'image_distribution': image_distribution,\n    } \n    return img_array, mask","metadata":{"execution":{"iopub.status.busy":"2022-09-26T21:31:12.413005Z","iopub.execute_input":"2022-09-26T21:31:12.413284Z","iopub.status.idle":"2022-09-26T21:31:12.425526Z","shell.execute_reply.started":"2022-09-26T21:31:12.413252Z","shell.execute_reply":"2022-09-26T21:31:12.424816Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def read_tf_dataset2(storage_file_path):\n    encoded_image_dataset = tf.data.TFRecordDataset(storage_file_path, compression_type=\"GZIP\")\n    parsed_image_dataset = encoded_image_dataset.map(_parse_image_function2)\n    return parsed_image_dataset","metadata":{"execution":{"iopub.status.busy":"2022-09-26T21:31:12.426457Z","iopub.execute_input":"2022-09-26T21:31:12.426728Z","iopub.status.idle":"2022-09-26T21:31:12.436222Z","shell.execute_reply.started":"2022-09-26T21:31:12.426695Z","shell.execute_reply":"2022-09-26T21:31:12.435474Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():   \n    model = unet_model()\n    model.summary()\n    model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=0.01),\n              loss=dice_loss,\n              metrics=[dice_coeff])","metadata":{"execution":{"iopub.status.busy":"2022-09-26T21:31:12.438967Z","iopub.execute_input":"2022-09-26T21:31:12.439211Z","iopub.status.idle":"2022-09-26T21:31:13.173322Z","shell.execute_reply.started":"2022-09-26T21:31:12.439182Z","shell.execute_reply":"2022-09-26T21:31:13.172576Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 512, 512, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 512, 512, 64) 1792        input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 512, 512, 64) 256         conv2d[0][0]                     \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 512, 512, 64) 36928       batch_normalization[0][0]        \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 256, 256, 64) 0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 256, 256, 128 73856       max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 256, 256, 128 512         conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 256, 256, 128 147584      batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 128 0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 128, 128, 256 295168      max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 128, 128, 256 590080      conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 64, 64, 512)  1180160     max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 64, 64, 512)  2359808     conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 512)  0           conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 32, 32, 1024) 4719616     max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 32, 32, 1024) 9438208     conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nup_sampling2d (UpSampling2D)    (None, 64, 64, 1024) 0           conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 64, 64, 512)  2097664     up_sampling2d[0][0]              \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 64, 64, 1024) 0           conv2d_7[0][0]                   \n                                                                 conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate[0][0]                \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_1 (UpSampling2D)  (None, 128, 128, 512 0           conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 128, 128, 256 524544      up_sampling2d_1[0][0]            \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 128, 128, 512 0           conv2d_5[0][0]                   \n                                                                 conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_1[0][0]              \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 128, 128, 256 590080      conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_2 (UpSampling2D)  (None, 256, 256, 256 0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 256, 256, 128 131200      up_sampling2d_2[0][0]            \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 256, 256, 256 0           conv2d_3[0][0]                   \n                                                                 conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 256, 256, 128 295040      concatenate_2[0][0]              \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 256, 256, 128 147584      conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_3 (UpSampling2D)  (None, 512, 512, 128 0           conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 512, 512, 64) 32832       up_sampling2d_3[0][0]            \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 512, 512, 128 0           conv2d_1[0][0]                   \n                                                                 conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_3[0][0]              \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 512, 512, 1)  65          conv2d_21[0][0]                  \n==================================================================================================\nTotal params: 31,032,513\nTrainable params: 31,032,129\nNon-trainable params: 384\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"with strategy.scope():\n    ACCELERATOR_TYPE = 'GPU'\n    if ACCELERATOR_TYPE == 'GPU':\n        batch_size = 128\n        train_dataset = read_tf_dataset2(train_data) \n        train_dataset = train_dataset.batch(batch_size, drop_remainder=True).cache().prefetch(2)\n        validation_dataset = read_tf_dataset2(validation_data)\n        validation_dataset = validation_dataset.batch(batch_size, drop_remainder=True).prefetch(2)\n        steps_per_epoch = 100\n        checkpointer = ModelCheckpoint('/kaggle/working/unet-gpu.h5', verbose=1)\n        history = model.fit(train_dataset,batch_size=batch_size, epochs=30, callbacks=[checkpointer])\n        model.save_weights(\"/kaggle/working/hubmap-gpu-unetf.h5\")\n\nhistory","metadata":{"execution":{"iopub.status.busy":"2022-09-26T21:31:13.174322Z","iopub.execute_input":"2022-09-26T21:31:13.177357Z","iopub.status.idle":"2022-09-26T21:31:14.890608Z","shell.execute_reply.started":"2022-09-26T21:31:13.177326Z","shell.execute_reply":"2022-09-26T21:31:14.889644Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"2022-09-26 21:31:13.418816: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n2022-09-26 21:31:13.422545: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000189999 Hz\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/1647595617.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/unet-gpu.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/hubmap-gpu-unetf.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: 2 root error(s) found.\n  (0) Not found:  0486052bb-anatomical-structure.json; No such file or directory\n\t [[node IteratorGetNext (defined at tmp/ipykernel_18/1647595617.py:11) ]]\n  (1) Not found:  0486052bb-anatomical-structure.json; No such file or directory\n\t [[node IteratorGetNext (defined at tmp/ipykernel_18/1647595617.py:11) ]]\n\t [[IteratorGetNext/_8]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2858]\n\nFunction call stack:\ntrain_function -> train_function\n"],"ename":"NotFoundError","evalue":"2 root error(s) found.\n  (0) Not found:  0486052bb-anatomical-structure.json; No such file or directory\n\t [[node IteratorGetNext (defined at tmp/ipykernel_18/1647595617.py:11) ]]\n  (1) Not found:  0486052bb-anatomical-structure.json; No such file or directory\n\t [[node IteratorGetNext (defined at tmp/ipykernel_18/1647595617.py:11) ]]\n\t [[IteratorGetNext/_8]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2858]\n\nFunction call stack:\ntrain_function -> train_function\n","output_type":"error"}]}]}